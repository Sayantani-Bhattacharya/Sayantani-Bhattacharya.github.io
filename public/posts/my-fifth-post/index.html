<!DOCTYPE html>
<html>

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=41363&amp;path=livereload" data-no-instant defer></script><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta http-equiv="Accept-CH" content="DPR, Viewport-Width, Width">
<link rel="icon" href=/home/sayantani/Documents/Sayantani-Bhattacharya.github.io/favOG.png type="image/gif">


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
      as="style"
      href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"
>
<link rel="stylesheet"
      href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"
      media="print" onload="this.media='all'" />
<noscript>
  <link
          href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"
          rel="stylesheet">
</noscript>


<link rel="stylesheet" href="/css/font.css" media="all">



<meta property="og:title" content="Human-Robot Teleoperation via mVAE: Predicting Robot Motion from Human EMG and IMU Signals" />
<meta property="og:description" content="Authors: Jialu Yu Motivation Rehabilitation and automation are two fields where robotics can make a real difference in people’s lives. For stroke patients undergoing upper-body rehabilitation, traditional exercises can feel repetitive and dull, making it hard to stay motivated. By adding robotics into the mix, we can turn these exercises into dynamic and engaging activities that not only boost engagement but also improve muscle activation and recovery. Similarly, in warehouses, many tasks still rely heavily on manual labor." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:41363/posts/my-fifth-post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-04T05:18:56-06:00" />
<meta property="article:modified_time" content="2024-12-04T05:18:56-06:00" /><meta property="og:site_name" content="Hugo Profile" />


<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Human-Robot Teleoperation via mVAE: Predicting Robot Motion from Human EMG and IMU Signals"/>
<meta name="twitter:description" content="Authors: Jialu Yu Motivation Rehabilitation and automation are two fields where robotics can make a real difference in people’s lives. For stroke patients undergoing upper-body rehabilitation, traditional exercises can feel repetitive and dull, making it hard to stay motivated. By adding robotics into the mix, we can turn these exercises into dynamic and engaging activities that not only boost engagement but also improve muscle activation and recovery. Similarly, in warehouses, many tasks still rely heavily on manual labor."/>


<link rel="stylesheet" href="/bootstrap-5/css/bootstrap.min.css" media="all"><link rel="stylesheet" href="/css/header.css" media="all">
<link rel="stylesheet" href="/css/footer.css" media="all">


<link rel="stylesheet" href="/css/theme.css" media="all">




<style>
    :root {
        --text-color: #343a40;
        --text-secondary-color: #6c757d;
        --background-color: #eaedf0;
        --secondary-background-color: #64ffda1a;
        --primary-color: #007bff;
        --secondary-color: #f8f9fa;

         
        --text-color-dark: #e4e6eb;
        --text-secondary-color-dark: #b0b3b8;
        --background-color-dark: #18191a;
        --secondary-background-color-dark: #212529;
        --primary-color-dark: #ffffff;
        --secondary-color-dark: #212529;
    }
    body {
        font-size: 1rem;
        font-weight: 400;
        line-height: 1.5;
        text-align: left;
    }

    html {
        background-color: var(--background-color) !important;
    }

    body::-webkit-scrollbar {
        width: .5em;
        height: .5em;
        background-color: var(--background-color);
    }
    
    ::-webkit-scrollbar-track {
        box-shadow: inset 0 0 6px var(--background-color);
        border-radius: 1rem;
    }
    
    ::-webkit-scrollbar-thumb {
        border-radius: 1rem;
        background-color: var(--secondary-color);
        outline: 1px solid var(--background-color);
    }

    #search-content::-webkit-scrollbar {
        width: .5em;
        height: .1em;
        background-color: var(--background-color);
    }
</style>

<meta name="description" content="">
<link rel="stylesheet" href="/css/single.css">


<script defer src="/fontawesome-6/all-6.4.2.js"></script>

  <title>
Human-Robot Teleoperation via mVAE: Predicting Robot Motion from Human EMG and IMU Signals | Sayantani&#39;s portfolio

  </title>
</head>

<body class="light">
  
  
<script>
    let localStorageValue = localStorage.getItem("pref-theme");
    let mediaQuery = window.matchMedia('(prefers-color-scheme: dark)').matches;

    switch (localStorageValue) {
        case "dark":
            document.body.classList.add('dark');
            break;
        case "light":
            document.body.classList.remove('dark');
            break;
        default:
            if (mediaQuery) {
                document.body.classList.add('dark');
            }
            break;
    }
</script>




<script>
    var prevScrollPos = window.pageYOffset;
    window.addEventListener("scroll", function showHeaderOnScroll() {
        let profileHeaderElem = document.getElementById("profileHeader");
        let currentScrollPos = window.pageYOffset;
        let resetHeaderStyle = false;
        let showNavBarOnScrollUp =  true ;
        let showNavBar = showNavBarOnScrollUp ? prevScrollPos > currentScrollPos : currentScrollPos > 0;
        if (showNavBar) {
            profileHeaderElem.classList.add("showHeaderOnTop");
        } else {
            resetHeaderStyle = true;
        }
        if(currentScrollPos === 0) {
            resetHeaderStyle = true;
        }
        if(resetHeaderStyle) {
            profileHeaderElem.classList.remove("showHeaderOnTop");
        }
        prevScrollPos = currentScrollPos;        
    });
</script>



<header id="profileHeader">
    <nav class="pt-3 navbar navbar-expand-lg animate">
        <div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5">
            
            <a class="navbar-brand primary-font text-wrap" href="/">
                
                Sayantani&#39;s portfolio
                
            </a>

            
                <div>
                    <input id="search" autocomplete="off" class="form-control mr-sm-2 d-none d-md-block" placeholder='Search'
                        aria-label="Search" oninput="searchOnChange(event)">
                </div>
            

            
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarContent"
                aria-controls="navbarContent" aria-expanded="false" aria-label="Toggle navigation">
                <svg aria-hidden="true" height="24" viewBox="0 0 16 16" version="1.1" width="24" data-view-component="true">
                    <path fill-rule="evenodd" d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z"></path>
                </svg>
            </button>

            
            <div class="collapse navbar-collapse text-wrap primary-font" id="navbarContent">
                <ul class="navbar-nav ms-auto text-center">
                    
                        <li class="nav-item navbar-text d-block d-md-none">
                            <div class="nav-link">
                                <input id="search" autocomplete="off" class="form-control mr-sm-2" placeholder='Search' aria-label="Search" oninput="searchOnChange(event)">
                            </div>
                        </li>
                    

                    

                    

                    

                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/#projects"
                            aria-label="projects">
                            Projects
                        </a>
                    </li>
                    

                    

                    

                    
                    
                    
                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/about" title="About Me">
                            
                            About Me
                        </a>
                    </li>
                    
                    
                    
                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/contact" title="Contact">
                            
                            Contact
                        </a>
                    </li>
                    
                    
                    
                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/publications" title="Publications">
                            
                            Publications
                        </a>
                    </li>
                    
                    
                    
                    
                    <li class="nav-item navbar-text">
                        <a class="nav-link" href="/resume" title="resume Link">
                            
                            Resume
                        </a>
                    </li>
                    
                    

                    
                    <li class="nav-item navbar-text">
                        
                        <div class="text-center">
                            <button id="theme-toggle">
                                <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                                </svg>
                                <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                    <circle cx="12" cy="12" r="5"></circle>
                                    <line x1="12" y1="1" x2="12" y2="3"></line>
                                    <line x1="12" y1="21" x2="12" y2="23"></line>
                                    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                                    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                                    <line x1="1" y1="12" x2="3" y2="12"></line>
                                    <line x1="21" y1="12" x2="23" y2="12"></line>
                                    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                                    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                                </svg>
                            </button>
                        </div>
                    </li>
                    

                </ul>

            </div>
        </div>
    </nav>
</header>
<div id="content">
<section id="single">
  <div class="container">
    <div class="row justify-content-center">
      <div class="col-sm-12 col-md-12 col-lg-9">
        <div class="pr-lg-4">
          <div class="title mb-5">
            <h1 class="text-center mb-4">Human-Robot Teleoperation via mVAE: Predicting Robot Motion from Human EMG and IMU Signals</h1>
            <div class="text-center">
               
              <small>|</small>
              Dec 4, 2024

              
              <span id="readingTime">
                min read
              </span>
              
            </div>
          </div>
          
          <article class="page-content  p-2">
          <div style="text-align:center;">Authors: Jialu Yu</div>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/YKh_bXhg4ik" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js">
</script>
<h2 id="motivation">Motivation</h2>
<p>Rehabilitation and automation are two fields where robotics can make a real difference in people’s lives. For stroke patients undergoing upper-body rehabilitation, traditional exercises can feel repetitive and dull, making it hard to stay motivated. By adding robotics into the mix, we can turn these exercises into dynamic and engaging activities that not only boost engagement but also improve muscle activation and recovery. Similarly, in warehouses, many tasks still rely heavily on manual labor. Human-controlled robotic systems offer an exciting way to combine human intuition with robotic precision, creating safer and more efficient workflows—even in challenging environments where signal loss or interference can occur.</p>
<p>This project was inspired by the need to bridge the gap between human intent and robotic action. By using Myo armbands to capture human arm movements and muscle activity through IMU (Inertial Measurement Unit) and EMG (Electromyography) signals, I trained a Multimodal Variational Autoencoder (mVAE) to predict and control a Franka Emika robotic arm. What makes the mVAE special is its ability to handle missing or incomplete data, ensuring the system remains reliable even in less-than-ideal conditions. This versatility makes it a promising solution for both rehabilitation and automation, where adaptability and precision are key.</p>
<h2 id="mission">Mission</h2>
<p>The objective of this project is to develop and deploy a robust machine learning framework that <strong>translates human arm movements</strong> into <strong>precise robotic control</strong>. Specifically, this includes:</p>
<ul>
<li>
<p><strong>Data Collection and Synchronization</strong> Capturing synchronized IMU and EMG signals from the Myo armbands, cube force sensor data, along with the robotic arm’s joint data, ensuring high-quality, multimodal input for training the model.</p>
</li>
<li>
<p><strong>Data process</strong> The data is processed through various filters and downsampling to ensure IMU, EMG, and robot joint data are corresponding with each other when feeding into the model.</p>
</li>
<li>
<p><strong>Model Training and Adaptation</strong> Designing and training a Multimodal Variational Autoencoder (mVAE) to predict and reconstruct robotic joint positions and velocities based on human input. The model’s robustness is enhanced through data augmentation techniques such as <em>time-step concatenation</em> and <em>input masking</em>, allowing it to perform reliably even with partial or noisy data.</p>
</li>
<li>
<p><strong>Real-Time Deployment</strong> Implementing the trained mVAE model to control a Franka Emika robotic arm in both simulated and real-world environments, achieving responsive and accurate robotic movements based on human intent.</p>
</li>
</ul>
<p>By achieving these objectives, the project demonstrates the feasibility of human-driven robotic control and its potential impact in diverse fields.</p>
<h2 id="data-collection">Data Collection</h2>
<p>The data collection process is divided into two parts: <strong>Human Arm Data Collection</strong> and <strong>Robot Data Collection</strong>. Both the human and robot perform the same series of tasks to collect their respective data, specifically: picking up and placing a cube.</p>
<h3 id="task-setup-and-board-design">Task Setup and Board Design</h3>
<div style="text-align: center;">
   <img src="/images/plate_itself.jpg" alt="targets" width="500"/>
</div>
The task is carried out on the board shown above, which includes one starting position (a square) and 27 target positions on the same plane. The target positions are arranged in a semi-circle, characterized by varying combinations of angles (0°, 22.5°, 45°, 67.5°, 90°, 112.5°, 135°, 157.5°, and 180°), distances (0.1m, 0.2m, 0.3m), and heights (0m and 0.078m). This setup creates a total of 54 distinct target positions (27 2D positions with 2 height variations), which leads to 54 raw datasets among all human, robot and cube data. 
<p>At the start of each trial, the cube is placed at the square-shaped starting position, and both the human and robot begin from their respective home positions. The task is to grab the cube and place it at a target position.</p>
<h3 id="myo-armband-for-human-data-collection">Myo Armband for Human Data Collection</h3>
<!-- ![targets](/images/Cube_with_force_sensor.png) -->
<div style="text-align: center;">
   <img src="/images/Cube_with_force_sensor.png" alt="targets" width="500"/>
</div>
During the human trials, two Myo armbands are used: one worn on the upper arm and the other on the lower arm, as illustrated below. These armbands are connected via serial Bluetooth, allowing for seamless collection of both IMU (Inertial Measurement Unit) and EMG (Electromyography) data through the ROS2 system.
<p>Each trial involves collecting data across the entire task cycle: starting from the initial position, grabbing the cube, moving to the target position, and finally placing the cube at the designated location. To monitor the gripping force, each side of the cube is equipped with a force sensor, which records force data during the task. To ensure robustness and reliability in the machine learning model, the entire task is repeated four times in each trial.</p>
<h3 id="cube-force-data-collection">Cube Force Data Collection</h3>
<p>A cube equipped with force sensors on each side is used during both the human and robot trials to measure the force applied to the cube. This data will be used to map the force applied by the human&rsquo;s fingers onto the robot’s gripper strength, which is an important aspect for accurate teleoperation control.</p>
<h3 id="robot-data-collection">Robot Data Collection</h3>
<p>For the robot trials, a Franka Emika robot performs the same task as the human subject. Starting from the same home position, the robot grabs the cube, moves to the target position, and places the cube there. Data collection is managed using ROS2 and <strong>rosbag</strong> to record the robot’s joint trajectories. The robot&rsquo;s joint positions and velocities are captured at each timestamp during the trial.</p>
<h2 id="data-processing">Data Processing</h2>
<p>The EMG data is sampled at 200 Hz, while the IMU data is sampled at 50 Hz, resulting in different data lengths within the same trial. To create a consistent dataset, it is necessary to align the EMG and IMU data with the robot joint velocity, position data, and the cube force sensor data.</p>
<h3 id="human-data-processing">Human Data Processing</h3>
<ol>
<li>
<p><strong>Segmentation</strong>:</p>
<p>The EMG data is first segmented based on valid movements during the task, which includes the following phases:</p>
<ul>
<li>From the start position to grabbing the cube</li>
<li>Moving the cube to the target position</li>
<li>Dropping the cube at the target position</li>
</ul>
<p>A clustering-based machine learning algorithm is employed to identify the boundaries of the movement phases, dividing the task into distinct segments. This process results in four segments of EMG and IMU data for each trial: one set from the upper Myo armband and another from the lower Myo armband. These segments are then combined to create a comprehensive dataset that captures both muscle activity and movement dynamics, as shown below.</p>
<div style="text-align: center;">
   <img src="/images/Combine_emg_imu_unprocessed.svg" alt="targets" width="600"/>
</div>
</li>
<li>
<p><strong>Data Smoothing and Rectification</strong>:<br>
Due to the noisy nature of the EMG data, a smoothing and rectification process is applied to improve its quality for machine learning. This helps ensure that the data is cleaner and more suitable for model training.</p>
</li>
<li>
<p><strong>Downsampling</strong>:<br>
Next, the EMG data is downsampled to match the time synchronization with the IMU data. Linear interpolation is used to ensure that both the EMG and IMU data segments are aligned in time.</p>
</li>
<li>
<p><strong>Alignment</strong>:<br>
After downsampling, the shortest data segment length among all EMG and IMU segments is identified. All other segments are downsampled to this length to ensure uniformity. At this point, each segment (EMG and IMU) will have the same length for each trial.</p>
</li>
<li>
<p><strong>Data Combination</strong>:<br>
The four segmented datasets (for both the upper and lower armbands) are then combined into four distinct datasets per trial:</p>
<ul>
<li>Upper Myo EMG combined data</li>
<li>Upper Myo IMU combined data</li>
<li>Lower Myo EMG combined data</li>
<li>Lower Myo IMU combined data</li>
</ul>
<p>Each dataset now has consistent data length, representing the human’s performance of the task across all four repetitions in a trial.</p>
<p>A set of one trial&rsquo;s (4 repetitions) EMG and IMU data is shown below.</p>
<div style="text-align: center;">
   <img src="/images/smoothed_imu_emg.svg" alt="targets" width="800"/>
</div>
</li>
</ol>
<h3 id="robot-data-processing">Robot Data Processing</h3>
<ol>
<li>
<p><strong>Segmentation</strong>:<br>
Similar to the human data, the robot data is segmented according to the desired task movements (grabbing, moving, and placing the cube).</p>
</li>
<li>
<p><strong>Downsampling and Alignment</strong>:<br>
The robot data is then downsampled to align with the human EMG and IMU data. A low-pass filter is applied to smooth the robot’s joint positions and velocities, ensuring that the data is consistent and suitable for training.</p>
</li>
<li>
<p><strong>Repetition</strong>:<br>
Since the robot performs the task once per trial (as the robot follows a fixed trajectory), it does not repeat the task like the human data. To align the robot data with the human data (which has been repeated four times), the robot’s data is duplicated four times to match the four repetitions of the human trial. This ensures that the robot’s trajectory is aligned with the four human data segments in each trial.</p>
</li>
</ol>
<p>A set of one trial&rsquo;s (4 repetitions) data and a comparison with one repetition of robot data are shown below.</p>
   <div style="text-align: center;">
      <img src="/images/Robot_data.png" alt="targets" width="800"/>
   </div>
## Data Processing Steps:
<h3 id="human-data">Human Data:</h3>
<ul>
<li>Segment and cluster EMG data based on task movements.</li>
<li>Apply smoothing and rectification to the EMG data.</li>
<li>Downsample and align EMG and IMU data.</li>
<li>Ensure uniform length across all data segments.</li>
<li>Combine the data for both upper and lower armbands.</li>
</ul>
<h3 id="robot-data">Robot Data:</h3>
<ul>
<li>Segment robot task movements and apply a low-pass filter.</li>
<li>Downsample robot data to match human data.</li>
<li>Repeat robot data to align with the four human task repetitions.</li>
</ul>
<h3 id="data-augmentation-process">Data Augmentation Process</h3>
<ol>
<li>
<p><strong>Normalization:</strong></p>
<ul>
<li>The original data is normalized to the range of [-1, 1] using <code>sklearn.preprocessing.MinMaxScaler</code>. This ensures consistent scaling across all features.</li>
</ul>
</li>
<li>
<p><strong>Time-Step Concatenation:</strong></p>
<ul>
<li>For each data point at the current time step ( t ), the data is horizontally concatenated with the corresponding data from the previous time step ( t-1 ). This captures temporal dependencies in the dataset.</li>
</ul>
</li>
<li>
<p><strong>Dataset Splitting:</strong></p>
<ul>
<li>The normalized dataset is split into a training set and a testing set using an 80:20 ratio to ensure an appropriate balance between training and evaluation.</li>
</ul>
</li>
<li>
<p><strong>Data Masking for Augmentation:</strong></p>
<ul>
<li>Two masked datasets are created:
<ul>
<li><strong>Case 2 Dataset:</strong> All robot data in the training set is masked with the value -2.</li>
<li><strong>Case 3 Dataset:</strong> All original data at time step ( t ) in the training set is masked with the value -2.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Final Augmented Training Set:</strong></p>
<ul>
<li>The original training data is vertically concatenated with the Case 2 and Case 3 datasets, resulting in the final augmented training set. This augmentation ensures the model is robust to incomplete or missing data during training.</li>
</ul>
</li>
</ol>
<h2 id="multimodal-variational-autoencoder-mvae-training-and-results">Multimodal Variational Autoencoder (mVAE) training and results</h2>
<h3 id="variational-autoencoder-vae">Variational Autoencoder (VAE)</h3>
<h3 id="variational-autoencoder-vae-1">Variational Autoencoder (VAE)</h3>
<p>A Variational Autoencoder (VAE) is a type of latent variable generative model, consisting of two primary components:</p>
<ul>
<li><strong>An Encoder:</strong> Transforms the input data ( x ) into a lower-dimensional latent representation ( z ), as follows:
$$
z = \text{encoder}(x)
$$</li>
<li><strong>A Decoder:</strong> Reconstructs the input data from the latent representation ( z ), such that:
$$
x = \text{decoder}(z)
$$</li>
</ul>
<p>The <strong>information bottleneck</strong> created by compressing the input into a lower-dimensional latent space results in some information loss. This loss is captured by the <strong>evidence lower bound (ELBO):</strong></p>
<p>$$
\text{ELBO} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \lambda \cdot KL[q(z|x) || p(z)]
$$</p>
<p>where:</p>
<ul>
<li>KL[q, p] represents the Kullback–Leibler (KL) divergence between the approximate posterior q(z|x) and the prior p(z).</li>
<li>λ and β are parameters used to balance reconstruction accuracy and latent space regularization.</li>
</ul>
<p>The ELBO is optimized during training using stochastic gradient descent, with the <strong>reparameterization trick</strong> applied to estimate gradients efficiently. For this study, the focus was placed on the model&rsquo;s <strong>reconstruction capability</strong>, so β was set to zero. By concentrating solely on the reconstruction loss, significant improvements in reconstruction performance were achieved.</p>
<hr>
<p>This project extends the standard VAE to multimodal data, forming a <strong>Multimodal Variational Autoencoder (mVAE)</strong>. The mVAE architecture includes:</p>
<ul>
<li><strong>Six Encoders and Decoders:</strong> Each corresponding to a specific sensory modality.
<ul>
<li>Each encoder and decoder operates as an independent neural network and does not share weights with other modalities&rsquo; networks.</li>
</ul>
</li>
<li><strong>A Shared Latent Representation:</strong> All encoders map their respective inputs (one sensory modality) into a shared latent code ( z ).</li>
</ul>
<p>The network architecture is depicted below:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Layer</th>
<th style="text-align:center">RL_IMU</th>
<th style="text-align:center">RL_EMG</th>
<th style="text-align:center">RU_IMU</th>
<th style="text-align:center">RU_EMG</th>
<th style="text-align:center">Robot_Joint_Pos</th>
<th style="text-align:center">Joint_Vel</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Input Layer</strong></td>
<td style="text-align:center"><strong>20 dims</strong></td>
<td style="text-align:center"><strong>16 dims</strong></td>
<td style="text-align:center"><strong>20 dims</strong></td>
<td style="text-align:center"><strong>16 dims</strong></td>
<td style="text-align:center"><strong>18 dims</strong></td>
<td style="text-align:center"><strong>18 dims</strong></td>
</tr>
<tr>
<td style="text-align:center"><strong>Modality Encoders Layer 1</strong></td>
<td style="text-align:center"><strong>100-ReLU</strong></td>
<td style="text-align:center"><strong>80-ReLU</strong></td>
<td style="text-align:center"><strong>100-ReLU</strong></td>
<td style="text-align:center"><strong>80-ReLU</strong></td>
<td style="text-align:center"><strong>90-ReLU</strong></td>
<td style="text-align:center"><strong>90-ReLU</strong></td>
</tr>
<tr>
<td style="text-align:center"><strong>Modality Encoders Layer 2</strong></td>
<td style="text-align:center"><strong>50-ReLU</strong></td>
<td style="text-align:center"><strong>40-ReLU</strong></td>
<td style="text-align:center"><strong>50-ReLU</strong></td>
<td style="text-align:center"><strong>40-ReLU</strong></td>
<td style="text-align:center"><strong>45-ReLU</strong></td>
<td style="text-align:center"><strong>45-ReLU</strong></td>
</tr>
<tr>
<td style="text-align:center"><strong>Concatenation to 270 Dimensions</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"><strong>270 Dimensions</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>Shared Encoder</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"><strong>350-ReLU</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>Latent Space</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"><strong>100-ReLU×2</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>Shared Decoder Layer 1</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"><strong>350-ReLU</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>Shared Decoder Layer 2</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"><strong>270-ReLU</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>Slicing</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"><strong>(50, 40, 50, 40, 45, 45)</strong></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center"><strong>Modality Decoders</strong></td>
<td style="text-align:center"><strong>100-ReLU</strong></td>
<td style="text-align:center"><strong>80-ReLU</strong></td>
<td style="text-align:center"><strong>100-ReLU</strong></td>
<td style="text-align:center"><strong>80-ReLU</strong></td>
<td style="text-align:center"><strong>90-ReLU</strong></td>
<td style="text-align:center"><strong>90-ReLU</strong></td>
</tr>
<tr>
<td style="text-align:center"><strong>Reconstructed Data</strong></td>
<td style="text-align:center"><strong>20-ReLU</strong></td>
<td style="text-align:center"><strong>16-ReLU</strong></td>
<td style="text-align:center"><strong>20-ReLU</strong></td>
<td style="text-align:center"><strong>16-ReLU</strong></td>
<td style="text-align:center"><strong>18-ReLU</strong></td>
<td style="text-align:center"><strong>18-ReLU</strong></td>
</tr>
</tbody>
</table>
<h3 id="training-parameters">Training parameters:</h3>
<ul>
<li><strong>learning rate:</strong> 0.0005</li>
<li><strong>batch size:</strong> 1440</li>
<li><strong>optimizer:</strong> Adam</li>
<li><strong>training epochs:</strong> 80000</li>
</ul>
<!-- ### Loss Components Analysis

The total loss consists of two main components:

1. **Reconstruction Loss:** 
   - Measures the negative log probability of the input under the reconstructed distribution. It represents the "nats" required to reconstruct the input from the latent space.

2. **Latent Loss:** 
   - Defined as the Kullback-Leibler (KL) divergence between the latent space distribution and a prior. It regularizes the latent space and reflects the "nats" required to transmit the latent distribution given the prior.

---

#### Observations

- **Reconstruction Loss:**
  - Dropped from **15 to -5** within the first **10,000 epochs**, indicating rapid improvement.
  - Reached **negative values** after **10,000 epochs**, fluctuating between **-5 and 5** due to stochastic batch sampling.

- **Latent Loss:**
  - Stabilized at **30000**, much larger than the reconstruction loss but scaled down by a coefficient "alpha," which approaches zero during training.

---

### Insights

- **Reconstruction Loss Fluctuation:** Likely caused by stochastic sampling; consider tuning batch size or using gradient clipping.
- **Latent Loss Scaling:** "Alpha" effectively balances reconstruction and regularization, ensuring a structured latent space.
- **Negative Reconstruction Loss:** Indicates potential overfitting; apply dropout or increase dataset size for better generalization. -->
<h3 id="training-testsing">Training testsing</h3>
<ul>
<li><strong>Performance Testing (Metric: MSE)</strong></li>
</ul>
<ol>
<li><strong>Test 1:</strong> Predict (reconstruct) robot data from complete original data.</li>
<li><strong>Test 2:</strong> Predict (reconstruct) robot data from human data only.</li>
<li><strong>Test 3:</strong> Predict (reconstruct) all data at ( t ) from all data at ( t - 1 ) only.</li>
<li><strong>Test 4:</strong> Predict (reconstruct) future robot data (at ( t + 1 )) from human data only.</li>
</ol>
<hr>
<h3 id="additional-tests">Additional Tests</h3>
<ul>
<li><strong>Test 5:</strong> Feed the human data only to predict (reconstruct) the complete data.</li>
<li><strong>Test 6:</strong> Feed the data at ( t ) only from the previously reconstructed data, then predict the future robot data (at ( t + 1 )).</li>
</ul>
<hr>
<h3 id="results-and-plots">Results and Plots</h3>
<p>The results for each test and the plots for <strong>Test 4</strong> (comparing the original position/velocity at ( t ) with the predicted position/velocity at ( t + 1 )) are shown below:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Test</th>
<th style="text-align:center">Joint_pos_cur (at t)</th>
<th style="text-align:center">Joint_vel_cur (at t)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>Test 1</strong></td>
<td style="text-align:center">0.0047</td>
<td style="text-align:center">0.0091</td>
</tr>
<tr>
<td style="text-align:left"><strong>Test 2</strong></td>
<td style="text-align:center">0.0414</td>
<td style="text-align:center">0.0384</td>
</tr>
<tr>
<td style="text-align:left"><strong>Test 3</strong></td>
<td style="text-align:center">0.00502</td>
<td style="text-align:center">0.011</td>
</tr>
<tr>
<td style="text-align:left"><strong>Test 4</strong></td>
<td style="text-align:center">0.0502</td>
<td style="text-align:center">0.0481</td>
</tr>
</tbody>
</table>
<div style="text-align: center;">
   <img src="/images/pred_act_80000p2.png" alt="targets" width="1000"/>
</div>
<h3 id="discussion">Discussion</h3>
<p>The ultimate objective of this project is to predict future robot motion by monitoring current human motion, as depicted in <strong>Test 4</strong>. However, conducting the previous three tests provides valuable insights into the performance of individual components.</p>
<ul>
<li>
<p><strong>Test 1 and Test 2:</strong></p>
<ul>
<li>Test 1 demonstrates that the prediction error for robot data is minimal when using complete data as input.</li>
<li>Test 2 shows that while the error increases when only human data is used as input, the model still achieves excellent performance for position prediction.</li>
</ul>
</li>
<li>
<p><strong>Test 3:</strong></p>
<ul>
<li>Predicting data at time ( t ) (both human and robot data) from data at time ( t - 1 ) yields performance comparable to Test 1.</li>
<li>This consistency suggests that a one-time-step shift at 10 Hz does not significantly alter the state. Future work could explore incorporating larger time shifts to better evaluate prediction performance across more dynamic scenarios.</li>
</ul>
</li>
<li>
<p><strong>Test 4:</strong></p>
<ul>
<li>This test combines the functions of Test 2 and Test 3, involving two stages of reconstruction:
<ol>
<li><strong>Initial Stage:</strong> Human data is used to reconstruct the data at time ( t ).</li>
<li><strong>Secondary Stage:</strong> The reconstructed data at time ( t ) is fed as input for predicting data at time ( t - 1 ), ultimately enabling prediction of data at ( t + 1 ).</li>
</ol>
</li>
<li>From the plots, it is evident that:
<ul>
<li><strong>Position Predictions:</strong> Closely align with the ground truth positions.</li>
<li><strong>Velocity Predictions:</strong> Match the general trend of ground truth values, although some extreme points in joints 1, 2, and 4 were missed by the model.</li>
<li>These missed points correspond to boundary values ((-1) or (1)) in the dataset, highlighting a potential area for improvement in the model&rsquo;s handling of edge cases.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Overall, these tests provide a comprehensive evaluation of the model’s prediction capabilities and outline areas for potential future enhancements.</p>
<h2 id="related-github-repositories">Related GitHub Repositories</h2>
<ul>
<li><a href="https://github.com/NuCapybara/ROS2_Myo_Franka">ROS2_Myo_Franka</a>: A repository for connecting Myo armbands to the Franka Emika robot, including data collection and integrating machine learning pipelines into robot control.</li>
<li><a href="https://github.com/NuCapybara/mVAE_robot_human_training">mVAE_robot_human_training</a>: A repository for training the processed robot and human data.</li>
</ul>

          </article>
        </div>
      </div>
      <div class="col-sm-12 col-md-12 col-lg-3">
        <div id="stickySideBar" class="sticky-sidebar">
          
          <aside class="toc">
              <h5>
                Table Of Contents
              </h5>
              <div class="toc-content">
                <nav id="TableOfContents">
  <ul>
    <li><a href="#motivation">Motivation</a></li>
    <li><a href="#mission">Mission</a></li>
    <li><a href="#data-collection">Data Collection</a>
      <ul>
        <li><a href="#task-setup-and-board-design">Task Setup and Board Design</a></li>
        <li><a href="#myo-armband-for-human-data-collection">Myo Armband for Human Data Collection</a></li>
        <li><a href="#cube-force-data-collection">Cube Force Data Collection</a></li>
        <li><a href="#robot-data-collection">Robot Data Collection</a></li>
      </ul>
    </li>
    <li><a href="#data-processing">Data Processing</a>
      <ul>
        <li><a href="#human-data-processing">Human Data Processing</a></li>
        <li><a href="#robot-data-processing">Robot Data Processing</a></li>
        <li><a href="#human-data">Human Data:</a></li>
        <li><a href="#robot-data">Robot Data:</a></li>
        <li><a href="#data-augmentation-process">Data Augmentation Process</a></li>
      </ul>
    </li>
    <li><a href="#multimodal-variational-autoencoder-mvae-training-and-results">Multimodal Variational Autoencoder (mVAE) training and results</a>
      <ul>
        <li><a href="#variational-autoencoder-vae">Variational Autoencoder (VAE)</a></li>
        <li><a href="#variational-autoencoder-vae-1">Variational Autoencoder (VAE)</a></li>
        <li><a href="#training-parameters">Training parameters:</a></li>
        <li><a href="#training-testsing">Training testsing</a></li>
        <li><a href="#additional-tests">Additional Tests</a></li>
        <li><a href="#results-and-plots">Results and Plots</a></li>
        <li><a href="#discussion">Discussion</a></li>
      </ul>
    </li>
    <li><a href="#related-github-repositories">Related GitHub Repositories</a></li>
  </ul>
</nav>
              </div>
          </aside>
          

          

          
          <aside class="social">
            <h5>Social</h5>
            <div class="social-content">
              <ul class="list-inline">
                <li class="list-inline-item text-center">
                  <a target="_blank" href="https://twitter.com/share?text=Human-Robot%20Teleoperation%20via%20mVAE%3a%20Predicting%20Robot%20Motion%20from%20Human%20EMG%20and%20IMU%20Signals&url=http%3a%2f%2flocalhost%3a41363%2fposts%2fmy-fifth-post%2f">
                    <i class="fab fa-twitter"></i>
                  </a>
                </li>
                <li class="list-inline-item text-center">
                  <a target="_blank" href="https://api.whatsapp.com/send?text=Human-Robot%20Teleoperation%20via%20mVAE%3a%20Predicting%20Robot%20Motion%20from%20Human%20EMG%20and%20IMU%20Signals: http%3a%2f%2flocalhost%3a41363%2fposts%2fmy-fifth-post%2f">
                    <i class="fab fa-whatsapp"></i>
                  </a>
                </li>
                <li class="list-inline-item text-center">
                  <a target="_blank" href='mailto:?subject=Human-Robot%20Teleoperation%20via%20mVAE%3a%20Predicting%20Robot%20Motion%20from%20Human%20EMG%20and%20IMU%20Signals&amp;body=Check%20out%20this%20site http%3a%2f%2flocalhost%3a41363%2fposts%2fmy-fifth-post%2f'>
                    <i class="fa fa-envelope"></i>
                  </a>
                </li>
              </ul>
            </div>
          </aside>
          
        </div>
      </div>
    </div>
    <div class="row">
      <div class="col-sm-12 col-md-12 col-lg-9 p-4">
        
      </div>
    </div>
  </div>
  <button class="p-2 px-3" onclick="topFunction()" id="topScroll">
    <i class="fas fa-angle-up"></i>
  </button>
</section>


<div class="progress">
  <div id="scroll-progress-bar" class="progress-bar" role="progressbar" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100"></div>
</div>
<Script src="/js/scrollProgressBar.js"></script>


<script>
  var topScroll = document.getElementById("topScroll");
  window.onscroll = function() {scrollFunction()};

  function scrollFunction() {
    if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
      topScroll.style.display = "block";
    } else {
      topScroll.style.display = "none";
    }
  }

  function topFunction() {
    document.body.scrollTop = 0;
    document.documentElement.scrollTop = 0;
  }

  
  let stickySideBarElem = document.getElementById("stickySideBar");
  let stickyNavBar =  true ;
  if(stickyNavBar) {
    let headerElem = document.getElementById("profileHeader");
    let headerHeight = headerElem.offsetHeight + 15;
    stickySideBarElem.style.top = headerHeight + "px";
  } else {
    stickySideBarElem.style.top = "50px";
  }
</script>


<script src="/js/readingTime.js"></script>



  </div><footer>
    
    

    <div class="text-center pt-2">
    
    <span class="px-1">
        <a href="https://github.com/Sayantani-Bhattacharya" aria-label="github">
            <img src="/images/github_logo.png" alt="GitHub" style="width: 3.4em; height: 3.4em; vertical-align: middle;">           
            
            
        </a>
    </span>
    

    
    <span class="px-1">
        <a href="https://www.linkedin.com/in/sayantani-bhattacharya-19a419175/" aria-label="linkedin">
        <img src="/images/linkedin_logo.png" alt="Gmail"  alt="Gmail Logo" style="width: 2.4em; height: 2.4em; vertical-align: middle;">
    </a>        
    

    

    
    <span class="px-1">
        <a href="mailto:sayantanibhattacharya2025@u.northwestern.edu?subject=Hello!&amp;body=Lets%20change%20the%20world%20together!" aria-label="email">
        <img src="/images/gmail_logo.png" alt="Gmail"  alt="Gmail Logo" style="width: 3.4em; height: 3.4em; vertical-align: middle;">
    </a>        
    

    

    
</div><div class="container py-4">
    <div class="row justify-content-center">
        <div class="col-md-4 text-center">
            
            &copy; 2024  Sayantani Bhattacharya
            <div class="text-secondary">
                Made with
                <span class="text-danger">
                    &#10084;
                </span>
                by Sayatani Bhattacharya
                 
            </div>
        </div>
    </div>
</div></footer><script src="/bootstrap-5/js/bootstrap.bundle.min.js"></script>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

    var tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]'))
    var tooltipList = tooltipTriggerList.map(function (tooltipTriggerEl) {
        return new bootstrap.Tooltip(tooltipTriggerEl)
    })

</script>


    <script src="/js/search.js"></script>











  <section id="search-content" class="py-2">
    <div class="container" id="search-results"></div>
  </section>
</body>

</html>