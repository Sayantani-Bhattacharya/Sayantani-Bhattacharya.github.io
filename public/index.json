[{"content":"\u003cbr\u003e\n\u003cli class=\"inline-block\"\u003e\n  \u003ca\n    target=\"_blank\"\n    class=\"align-middle link-primary mr-2 mr-lg-0 ml-lg-2\"\n    href=\"/docs/Sayantani_Bhattacharya_Resume.pdf\"\n    \u003e\n    Download\n    \u003c/a\u003e\n\u003c/li\u003e\n\u003cbr\u003e\n\u003cobject data=\"/docs/Sayantani_Bhattacharya_Resume.pdf\" width=\"1200\" height=\"1200\" type=\"application/pdf\"\u003e\u003c/object\u003e","description":null,"image":null,"permalink":"http://localhost:1313/posts/resume/","title":"Resume"},{"content":"\u003c!-- \u003cdiv style=\"text-align:center;\"\u003eAuthor: Sayantani Bhattacharya\u003c/div\u003e --\u003e\n\u003c!-- \u003cdiv style=\"text-align: center; margin-bottom: 30px;\"\u003e\n    \u003cimg src=\"/images/projects/Quadruped_Fleet/QuadrupedFleet.gif\" alt=\"Robot Fleet\" width=\"200\" height=\"auto\"\u003e\n\u003c/div\u003e  --\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003c!-- Explanation of the system --\u003e\n\u003cp\u003eIn times of disaster, every second counts, and reaching survivors in hazardous terrains poses significant challenges.\nImagine a coordinated team of agile, four-legged robots, working together to navigate treacherous environments like dense forests or mines.\nThese quadrupedal robots autonomously perform simultaneous localization and mapping (SLAM), creating real-time detailed maps of their\nsurroundings. By employing decentralized collaborative system, these robots can share and merge their individual maps,\ncreating a comprehensive understanding of the complete area without relying on a central system.\nThis approach enhances the robustness and speed of search operations, as the failure of a single unit does not compromise the entire mission.\nQuadrupeds inherently work well in uneven terrains, and harnessing the strengths of SLAM to explore unmapped areas with LIDAR and Visual-Inertial sensor data, these robotic swarms represent\na leap forward in disaster response, offering hope and assistance when it\u0026rsquo;s needed most.\nBy all means this is just the first iteration and needs good work for being deployable onsite.\u003c/p\u003e\n\u003c!-- The block diagram --\u003e\n\u003cdiv style=\"text-align: center; margin-bottom: 30px;\"\u003e\n    \u003cimg src=\"/images/projects/Quadruped_Fleet/system.png\" alt=\"Robot Fleet\" width=\"900\" height=\"auto\"\u003e\n\u003c/div\u003e \n\u003ch2 id=\"unitree-go2\"\u003eUnitree-GO2\u003c/h2\u003e\n\u003c!-- A. hardware of both\n    B. block diagram of both / rqt_graph \n    C. manual nav, filetring data and high level controls.\n    D. Rviz videos both both. --\u003e\n\u003ch2 id=\"unitree-go1\"\u003eUnitree-GO1\u003c/h2\u003e\n\u003c!-- A. hardware of both\n    A. JEtson- edge computing\n    B. block diagram of both.\n    C. manual nav, filetring data and high level controls.\n    D. Rviz videos both both. --\u003e\n\u003ch2 id=\"visual-slam\"\u003eVisual-SLAM\u003c/h2\u003e\n \u003c!-- A. Theory\n        B. videos, point cloud, mapping.\n        C. sterio theory, Zed 2i cam --\u003e\n\u003ch2 id=\"lidar-slam\"\u003eLidar SLam\u003c/h2\u003e\n\u003c!-- A. theory\n        B.  videos, point cloud, mapping.\n        C. point cloud theory, 4D lidar stuff --\u003e\n\u003ch2 id=\"merging-map\"\u003eMerging Map\u003c/h2\u003e\n\u003c!-- a. theory\n        b. simulation video,... later complete thing --\u003e\n\u003ch2 id=\"future-work\"\u003eFuture Work\u003c/h2\u003e\n\u003ch2 id=\"points-to-take-of-while-working-on-it\"\u003ePoints to take of while working on it:\u003c/h2\u003e\n\u003c!-- Feel free to raise pr. Most issues, and operational stuff is fresh in my mind, so feel free to ask and add issues.\n\nLink to github. || Detailed setup instruction and running procedure is mentioned in the ReadMe files of each package. --\u003e\n\u003c!--  Doubts\n-\u003e All node/ launch file descriptions?\n-\u003e include that as another project? \n","description":null,"image":null,"permalink":"http://localhost:1313/posts/quadruped-fleet/","title":"Collaborative Hetrogeneous Quadruped Fleet"},{"content":"\u003c!-- \u003cdiv style=\"text-align:center;\"\u003eAuthor: Sayantani Bhattacharya\u003c/div\u003e --\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003ch2 id=\"overall-system-and-reference-frames\"\u003eOverall System and Reference Frames\u003c/h2\u003e\n\u003cp\u003eAttached is a detailed drawing of the system, illustrating all the frames in use, complete with frame labels. These frames and their labels have been consistently used and identified in the code to maintain clarity and coherence.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/jack.jpeg\" alt=\"targets\"\u003e\n\u003cbr /\u003e\n\u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFrames\u003c/strong\u003e\n\u003cbr /\u003e\nThe base frame for the box is called g_wf, where f is the center of the box.  \u003ccode\u003eg_f6, g_f7, g_f8, g_f9\u003c/code\u003e are the relative frames based on the box center. The frame between world and box vertices can be generated through cross product of \u003ccode\u003eg_wf\u003c/code\u003e and \u003ccode\u003eg_f6, g_f7, g_f8, g_f9\u003c/code\u003e.\nThe base frame for the jack is called g_wj, where j is the center of the jack.  \u003ccode\u003eg_jb,  g_ja, g_jc, g_jd\u003c/code\u003e are the relative frames based on the box center. The frame between world and box vertices can be generated through cross product of \u003ccode\u003eg_wj and g_jb,  g_ja, g_jc, g_jd\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEuler-Lagrange Equations\u003c/strong\u003e\n\u003cbr /\u003e\nThe Euler Lagrange equation is calculated based on the:\u003c/p\u003e\n\u003cp\u003eConsidering uniform mass distribution in both box and jack, I calculate the kinetic and potential energy at the centroid (geometric center of the two rectangles). And Lagrangian as kinetic minus the potential energy of the two bodies.\u003ccode\u003eL = KE - V\u003c/code\u003e, where \u003ccode\u003eKE\u003c/code\u003e is kinetic energy of both jack and box and \u003ccode\u003eV\u003c/code\u003e is the potential energy of both jack and box. Using the Euler lagrange formula to get the equations of motion, by equating it to the external force applied on both the rigid bodies.\u003c/p\u003e\n\u003cp\u003eThe python script calculating the kinetic energy \u003ccode\u003eKE\u003c/code\u003e and potential energy \u003ccode\u003eV\u003c/code\u003e is shown below:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#Kinetic Energy and Potential Energy\n#frame kinetic energy\nKE_frame = sym.simplify(0.5*(v_wf).T*Inertia_frame_mat*v_wf)\n  #jack kinetic energy\nKE_vwa = sym.simplify(0.5*(v_wa).T*Inertia_jack_mat*v_wa)\nKE_vwb = sym.simplify(0.5*(v_wb).T*Inertia_jack_mat*v_wb)\nKE_vwc = sym.simplify(0.5*(v_wc).T*Inertia_jack_mat*v_wc)\nKE_vwd = sym.simplify(0.5*(v_wd).T*Inertia_jack_mat*v_wd)\nKE_jack = sym.simplify(KE_vwa + KE_vwb + KE_vwc + KE_vwd)\n\n\nKE_total = sym.simplify(KE_jack+ KE_frame)\n\n\nhf = sym.simplify(sym.Matrix([g_wf[1,3]]))\nh_wa = sym.simplify(sym.Matrix([g_wa[1,3]]))\nh_wb = sym.simplify(sym.Matrix([g_wb[1,3]]))\nh_wc = sym.simplify(sym.Matrix([g_wc[1,3]]))\nh_wd = sym.simplify(sym.Matrix([g_wd[1,3]]))\n\n\nV_f = Mf*gravity*hf[0]\nV_wa =  m_j_point*gravity*h_wa[0]\nV_wb =  m_j_point*gravity*h_wb[0]\nV_wc =  m_j_point*gravity*h_wc[0]\nV_wd =  m_j_point*gravity*h_wd[0]\n\n\nV_total = sym.simplify(sym.Matrix([V_f + V_wa + V_wb + V_wc + V_wd]))\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eSystem variables:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDefine system variables as \u003ccode\u003eq = [x_f,  y_f, theta_f, x_j,  y_j,  theta_j]\u003c/code\u003e and use \u003ccode\u003eLagrangian Equation\u003c/code\u003e and \u003ccode\u003eq\u003c/code\u003e to calculate \u003ccode\u003eEuler-Lagrangian Equation\u003c/code\u003e.\u003cbr /\u003e\u003c/p\u003e\n\u003c!-- $$\\frac{\\partial L}{\\partial \\dot{q}} \\bigg |^{\\tau +}_{\\tau -} = \\lambda \\frac{\\partial \\phi}{\\partial q^{'}}$$ --\u003e\n\u003cp\u003e\u003cstrong\u003eExternal force:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe box is supposed to vibrate and have some degree of gravity compensation, so I have given the external force in y as 4\u003cem\u003eM_box\u003c/em\u003eg  [ M_box*g : is the force due to gravity, and 4 is a tunable parameter, could be any hard-set value, but its easier to reference wrt to gravitational force, so used this.] And the external force on theta is a sinusoidal function, to generate an oscillation/ vibration like motion. And there is no external force to jack (it has potential component, but that is internal). \u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eImpact conditions:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThere are 16 impact conditions in this model. Which is that every 4 point of the jack coming in contact with any of the boxâ€™s walls. Which basically means that the x component of transform (between corner and wall) for Wall 1 and 3 and y coordinate for Wall 2 and 4, should be less than a threshold value. Value should not be kept 0, as in simulation, till the 0 transform is actually achieved the animation would have stepped outside the box, as it is a bit coarsely discrete system. Also, this threshold value can be tuned. And I used it to detect the impact occurrence and calling the impact update function. \u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eImpact update procedure:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo find the state of the system after impact, we solve six conservation of momentum equations and one Hamiltonian conservation equation. We have 7 variables (lambda, and six state variables [q]) and 7 equations. And I used this to generate the trajectory after every impact. \u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSimulated trajectory plots:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBox-Motion:\n\u003cbr /\u003e\n\u003cimg src=\"/images/Box-Motion.png\" alt=\"targets\"\u003e\n\u003cbr /\u003e\n\u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003eBox-Velocity:\n\u003cbr /\u003e\n\u003cimg src=\"/images/Box-Velocity.png\" alt=\"targets\"\u003e\n\u003cbr /\u003e\n\u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003eJack-Motion:\n\u003cbr /\u003e\n\u003cimg src=\"/images/Jack-Motion.png\" alt=\"targets\"\u003e\n\u003cbr /\u003e\n\u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003eJack-Velocity:\n\u003cbr /\u003e\n\u003cimg src=\"/images/Jack-Velocity.png\" alt=\"targets\"\u003e\n\u003cbr /\u003e\n\u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSimulation Behavior Analysis:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhen any of the four corners of the jack comes in contact with any of the four walls of the box [ i.e. impact condition is met], the jack bounces off the wall and keeping the speed of jack same, as I am assuming elastic collision [ i.e. following the impact update rule].\u003c/p\u003e\n\u003cp\u003eAlso, the external force ensures the box does not free fall under gravity. If you uncomment the external force section, this behavior can be seen in the simulation as well. Also, as a sanity check, one may uncomment impact equations, and it would be observed that, both the bodies would have free fall decoupled from each other. \u003cbr /\u003e\u003c/p\u003e\n\u003c!-- $$\\bigg [ \\frac{\\partial L}{\\partial \\dot{q}} \\cdot \\dot{q} - L(q, \\dot{q}) \\bigg] ^{\\tau +}_{\\tau -} = 0.$$ --\u003e","description":null,"image":null,"permalink":"http://localhost:1313/posts/jack-in-box/","title":"Simulating the Dynamics from Scratch-Modeling of a Jack Bouncing within the Box Boundaries."},{"content":"\u003cdiv style=\"text-align:center;\"\u003eAuthors: Jialu Yu\u003c/div\u003e\n\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n  \u003ciframe src=\"https://www.youtube.com/embed/YKh_bXhg4ik\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen title=\"YouTube Video\"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" async\n  src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js\"\u003e\n\u003c/script\u003e\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eRehabilitation and automation are two fields where robotics can make a real difference in peopleâ€™s lives. For stroke patients undergoing upper-body rehabilitation, traditional exercises can feel repetitive and dull, making it hard to stay motivated. By adding robotics into the mix, we can turn these exercises into dynamic and engaging activities that not only boost engagement but also improve muscle activation and recovery. Similarly, in warehouses, many tasks still rely heavily on manual labor. Human-controlled robotic systems offer an exciting way to combine human intuition with robotic precision, creating safer and more efficient workflowsâ€”even in challenging environments where signal loss or interference can occur.\u003c/p\u003e\n\u003cp\u003eThis project was inspired by the need to bridge the gap between human intent and robotic action. By using Myo armbands to capture human arm movements and muscle activity through IMU (Inertial Measurement Unit) and EMG (Electromyography) signals, I trained a Multimodal Variational Autoencoder (mVAE) to predict and control a Franka Emika robotic arm. What makes the mVAE special is its ability to handle missing or incomplete data, ensuring the system remains reliable even in less-than-ideal conditions. This versatility makes it a promising solution for both rehabilitation and automation, where adaptability and precision are key.\u003c/p\u003e\n\u003ch2 id=\"mission\"\u003eMission\u003c/h2\u003e\n\u003cp\u003eThe objective of this project is to develop and deploy a robust machine learning framework that \u003cstrong\u003etranslates human arm movements\u003c/strong\u003e into \u003cstrong\u003eprecise robotic control\u003c/strong\u003e. Specifically, this includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData Collection and Synchronization\u003c/strong\u003e Capturing synchronized IMU and EMG signals from the Myo armbands, cube force sensor data, along with the robotic armâ€™s joint data, ensuring high-quality, multimodal input for training the model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData process\u003c/strong\u003e The data is processed through various filters and downsampling to ensure IMU, EMG, and robot joint data are corresponding with each other when feeding into the model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eModel Training and Adaptation\u003c/strong\u003e Designing and training a Multimodal Variational Autoencoder (mVAE) to predict and reconstruct robotic joint positions and velocities based on human input. The modelâ€™s robustness is enhanced through data augmentation techniques such as \u003cem\u003etime-step concatenation\u003c/em\u003e and \u003cem\u003einput masking\u003c/em\u003e, allowing it to perform reliably even with partial or noisy data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eReal-Time Deployment\u003c/strong\u003e Implementing the trained mVAE model to control a Franka Emika robotic arm in both simulated and real-world environments, achieving responsive and accurate robotic movements based on human intent.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy achieving these objectives, the project demonstrates the feasibility of human-driven robotic control and its potential impact in diverse fields.\u003c/p\u003e\n\u003ch2 id=\"data-collection\"\u003eData Collection\u003c/h2\u003e\n\u003cp\u003eThe data collection process is divided into two parts: \u003cstrong\u003eHuman Arm Data Collection\u003c/strong\u003e and \u003cstrong\u003eRobot Data Collection\u003c/strong\u003e. Both the human and robot perform the same series of tasks to collect their respective data, specifically: picking up and placing a cube.\u003c/p\u003e\n\u003ch3 id=\"task-setup-and-board-design\"\u003eTask Setup and Board Design\u003c/h3\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n   \u003cimg src=\"/images/plate_itself.jpg\" alt=\"targets\" width=\"500\"/\u003e\n\u003c/div\u003e\nThe task is carried out on the board shown above, which includes one starting position (a square) and 27 target positions on the same plane. The target positions are arranged in a semi-circle, characterized by varying combinations of angles (0Â°, 22.5Â°, 45Â°, 67.5Â°, 90Â°, 112.5Â°, 135Â°, 157.5Â°, and 180Â°), distances (0.1m, 0.2m, 0.3m), and heights (0m and 0.078m). This setup creates a total of 54 distinct target positions (27 2D positions with 2 height variations), which leads to 54 raw datasets among all human, robot and cube data. \n\u003cp\u003eAt the start of each trial, the cube is placed at the square-shaped starting position, and both the human and robot begin from their respective home positions. The task is to grab the cube and place it at a target position.\u003c/p\u003e\n\u003ch3 id=\"myo-armband-for-human-data-collection\"\u003eMyo Armband for Human Data Collection\u003c/h3\u003e\n\u003c!-- ![targets](/images/Cube_with_force_sensor.png) --\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n   \u003cimg src=\"/images/Cube_with_force_sensor.png\" alt=\"targets\" width=\"500\"/\u003e\n\u003c/div\u003e\nDuring the human trials, two Myo armbands are used: one worn on the upper arm and the other on the lower arm, as illustrated below. These armbands are connected via serial Bluetooth, allowing for seamless collection of both IMU (Inertial Measurement Unit) and EMG (Electromyography) data through the ROS2 system.\n\u003cp\u003eEach trial involves collecting data across the entire task cycle: starting from the initial position, grabbing the cube, moving to the target position, and finally placing the cube at the designated location. To monitor the gripping force, each side of the cube is equipped with a force sensor, which records force data during the task. To ensure robustness and reliability in the machine learning model, the entire task is repeated four times in each trial.\u003c/p\u003e\n\u003ch3 id=\"cube-force-data-collection\"\u003eCube Force Data Collection\u003c/h3\u003e\n\u003cp\u003eA cube equipped with force sensors on each side is used during both the human and robot trials to measure the force applied to the cube. This data will be used to map the force applied by the human\u0026rsquo;s fingers onto the robotâ€™s gripper strength, which is an important aspect for accurate teleoperation control.\u003c/p\u003e\n\u003ch3 id=\"robot-data-collection\"\u003eRobot Data Collection\u003c/h3\u003e\n\u003cp\u003eFor the robot trials, a Franka Emika robot performs the same task as the human subject. Starting from the same home position, the robot grabs the cube, moves to the target position, and places the cube there. Data collection is managed using ROS2 and \u003cstrong\u003erosbag\u003c/strong\u003e to record the robotâ€™s joint trajectories. The robot\u0026rsquo;s joint positions and velocities are captured at each timestamp during the trial.\u003c/p\u003e\n\u003ch2 id=\"data-processing\"\u003eData Processing\u003c/h2\u003e\n\u003cp\u003eThe EMG data is sampled at 200 Hz, while the IMU data is sampled at 50 Hz, resulting in different data lengths within the same trial. To create a consistent dataset, it is necessary to align the EMG and IMU data with the robot joint velocity, position data, and the cube force sensor data.\u003c/p\u003e\n\u003ch3 id=\"human-data-processing\"\u003eHuman Data Processing\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSegmentation\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003eThe EMG data is first segmented based on valid movements during the task, which includes the following phases:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFrom the start position to grabbing the cube\u003c/li\u003e\n\u003cli\u003eMoving the cube to the target position\u003c/li\u003e\n\u003cli\u003eDropping the cube at the target position\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA clustering-based machine learning algorithm is employed to identify the boundaries of the movement phases, dividing the task into distinct segments. This process results in four segments of EMG and IMU data for each trial: one set from the upper Myo armband and another from the lower Myo armband. These segments are then combined to create a comprehensive dataset that captures both muscle activity and movement dynamics, as shown below.\u003c/p\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n   \u003cimg src=\"/images/Combine_emg_imu_unprocessed.svg\" alt=\"targets\" width=\"600\"/\u003e\n\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData Smoothing and Rectification\u003c/strong\u003e:\u003cbr\u003e\nDue to the noisy nature of the EMG data, a smoothing and rectification process is applied to improve its quality for machine learning. This helps ensure that the data is cleaner and more suitable for model training.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDownsampling\u003c/strong\u003e:\u003cbr\u003e\nNext, the EMG data is downsampled to match the time synchronization with the IMU data. Linear interpolation is used to ensure that both the EMG and IMU data segments are aligned in time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAlignment\u003c/strong\u003e:\u003cbr\u003e\nAfter downsampling, the shortest data segment length among all EMG and IMU segments is identified. All other segments are downsampled to this length to ensure uniformity. At this point, each segment (EMG and IMU) will have the same length for each trial.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData Combination\u003c/strong\u003e:\u003cbr\u003e\nThe four segmented datasets (for both the upper and lower armbands) are then combined into four distinct datasets per trial:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUpper Myo EMG combined data\u003c/li\u003e\n\u003cli\u003eUpper Myo IMU combined data\u003c/li\u003e\n\u003cli\u003eLower Myo EMG combined data\u003c/li\u003e\n\u003cli\u003eLower Myo IMU combined data\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach dataset now has consistent data length, representing the humanâ€™s performance of the task across all four repetitions in a trial.\u003c/p\u003e\n\u003cp\u003eA set of one trial\u0026rsquo;s (4 repetitions) EMG and IMU data is shown below.\u003c/p\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n   \u003cimg src=\"/images/smoothed_imu_emg.svg\" alt=\"targets\" width=\"800\"/\u003e\n\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"robot-data-processing\"\u003eRobot Data Processing\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSegmentation\u003c/strong\u003e:\u003cbr\u003e\nSimilar to the human data, the robot data is segmented according to the desired task movements (grabbing, moving, and placing the cube).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDownsampling and Alignment\u003c/strong\u003e:\u003cbr\u003e\nThe robot data is then downsampled to align with the human EMG and IMU data. A low-pass filter is applied to smooth the robotâ€™s joint positions and velocities, ensuring that the data is consistent and suitable for training.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eRepetition\u003c/strong\u003e:\u003cbr\u003e\nSince the robot performs the task once per trial (as the robot follows a fixed trajectory), it does not repeat the task like the human data. To align the robot data with the human data (which has been repeated four times), the robotâ€™s data is duplicated four times to match the four repetitions of the human trial. This ensures that the robotâ€™s trajectory is aligned with the four human data segments in each trial.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA set of one trial\u0026rsquo;s (4 repetitions) data and a comparison with one repetition of robot data are shown below.\u003c/p\u003e\n   \u003cdiv style=\"text-align: center;\"\u003e\n      \u003cimg src=\"/images/Robot_data.png\" alt=\"targets\" width=\"800\"/\u003e\n   \u003c/div\u003e\n## Data Processing Steps:\n\u003ch3 id=\"human-data\"\u003eHuman Data:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSegment and cluster EMG data based on task movements.\u003c/li\u003e\n\u003cli\u003eApply smoothing and rectification to the EMG data.\u003c/li\u003e\n\u003cli\u003eDownsample and align EMG and IMU data.\u003c/li\u003e\n\u003cli\u003eEnsure uniform length across all data segments.\u003c/li\u003e\n\u003cli\u003eCombine the data for both upper and lower armbands.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"robot-data\"\u003eRobot Data:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSegment robot task movements and apply a low-pass filter.\u003c/li\u003e\n\u003cli\u003eDownsample robot data to match human data.\u003c/li\u003e\n\u003cli\u003eRepeat robot data to align with the four human task repetitions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"data-augmentation-process\"\u003eData Augmentation Process\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eNormalization:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe original data is normalized to the range of [-1, 1] using \u003ccode\u003esklearn.preprocessing.MinMaxScaler\u003c/code\u003e. This ensures consistent scaling across all features.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTime-Step Concatenation:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor each data point at the current time step ( t ), the data is horizontally concatenated with the corresponding data from the previous time step ( t-1 ). This captures temporal dependencies in the dataset.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDataset Splitting:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe normalized dataset is split into a training set and a testing set using an 80:20 ratio to ensure an appropriate balance between training and evaluation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData Masking for Augmentation:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTwo masked datasets are created:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCase 2 Dataset:\u003c/strong\u003e All robot data in the training set is masked with the value -2.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCase 3 Dataset:\u003c/strong\u003e All original data at time step ( t ) in the training set is masked with the value -2.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFinal Augmented Training Set:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe original training data is vertically concatenated with the Case 2 and Case 3 datasets, resulting in the final augmented training set. This augmentation ensures the model is robust to incomplete or missing data during training.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"multimodal-variational-autoencoder-mvae-training-and-results\"\u003eMultimodal Variational Autoencoder (mVAE) training and results\u003c/h2\u003e\n\u003ch3 id=\"variational-autoencoder-vae\"\u003eVariational Autoencoder (VAE)\u003c/h3\u003e\n\u003ch3 id=\"variational-autoencoder-vae-1\"\u003eVariational Autoencoder (VAE)\u003c/h3\u003e\n\u003cp\u003eA Variational Autoencoder (VAE) is a type of latent variable generative model, consisting of two primary components:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAn Encoder:\u003c/strong\u003e Transforms the input data ( x ) into a lower-dimensional latent representation ( z ), as follows:\n$$\nz = \\text{encoder}(x)\n$$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eA Decoder:\u003c/strong\u003e Reconstructs the input data from the latent representation ( z ), such that:\n$$\nx = \\text{decoder}(z)\n$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cstrong\u003einformation bottleneck\u003c/strong\u003e created by compressing the input into a lower-dimensional latent space results in some information loss. This loss is captured by the \u003cstrong\u003eevidence lower bound (ELBO):\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e$$\n\\text{ELBO} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\lambda \\cdot KL[q(z|x) || p(z)]\n$$\u003c/p\u003e\n\u003cp\u003ewhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKL[q, p] represents the Kullbackâ€“Leibler (KL) divergence between the approximate posterior q(z|x) and the prior p(z).\u003c/li\u003e\n\u003cli\u003eÎ» and Î² are parameters used to balance reconstruction accuracy and latent space regularization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe ELBO is optimized during training using stochastic gradient descent, with the \u003cstrong\u003ereparameterization trick\u003c/strong\u003e applied to estimate gradients efficiently. For this study, the focus was placed on the model\u0026rsquo;s \u003cstrong\u003ereconstruction capability\u003c/strong\u003e, so Î² was set to zero. By concentrating solely on the reconstruction loss, significant improvements in reconstruction performance were achieved.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eThis project extends the standard VAE to multimodal data, forming a \u003cstrong\u003eMultimodal Variational Autoencoder (mVAE)\u003c/strong\u003e. The mVAE architecture includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eSix Encoders and Decoders:\u003c/strong\u003e Each corresponding to a specific sensory modality.\n\u003cul\u003e\n\u003cli\u003eEach encoder and decoder operates as an independent neural network and does not share weights with other modalities\u0026rsquo; networks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eA Shared Latent Representation:\u003c/strong\u003e All encoders map their respective inputs (one sensory modality) into a shared latent code ( z ).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe network architecture is depicted below:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align:center\"\u003eLayer\u003c/th\u003e\n\u003cth style=\"text-align:center\"\u003eRL_IMU\u003c/th\u003e\n\u003cth style=\"text-align:center\"\u003eRL_EMG\u003c/th\u003e\n\u003cth style=\"text-align:center\"\u003eRU_IMU\u003c/th\u003e\n\u003cth style=\"text-align:center\"\u003eRU_EMG\u003c/th\u003e\n\u003cth style=\"text-align:center\"\u003eRobot_Joint_Pos\u003c/th\u003e\n\u003cth style=\"text-align:center\"\u003eJoint_Vel\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eInput Layer\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e20 dims\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e16 dims\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e20 dims\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e16 dims\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e18 dims\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e18 dims\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eModality Encoders Layer 1\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e100-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e80-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e100-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e80-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e90-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e90-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eModality Encoders Layer 2\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e50-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e40-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e50-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e40-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e45-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e45-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eConcatenation to 270 Dimensions\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e270 Dimensions\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eShared Encoder\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e350-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eLatent Space\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e100-ReLUÃ—2\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eShared Decoder Layer 1\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e350-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eShared Decoder Layer 2\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e270-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eSlicing\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e(50, 40, 50, 40, 45, 45)\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eModality Decoders\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e100-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e80-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e100-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e80-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e90-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e90-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003eReconstructed Data\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e20-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e16-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e20-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e16-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e18-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e\u003cstrong\u003e18-ReLU\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3 id=\"training-parameters\"\u003eTraining parameters:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003elearning rate:\u003c/strong\u003e 0.0005\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ebatch size:\u003c/strong\u003e 1440\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eoptimizer:\u003c/strong\u003e Adam\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003etraining epochs:\u003c/strong\u003e 80000\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ### Loss Components Analysis\n\nThe total loss consists of two main components:\n\n1. **Reconstruction Loss:** \n   - Measures the negative log probability of the input under the reconstructed distribution. It represents the \"nats\" required to reconstruct the input from the latent space.\n\n2. **Latent Loss:** \n   - Defined as the Kullback-Leibler (KL) divergence between the latent space distribution and a prior. It regularizes the latent space and reflects the \"nats\" required to transmit the latent distribution given the prior.\n\n---\n\n#### Observations\n\n- **Reconstruction Loss:**\n  - Dropped from **15 to -5** within the first **10,000 epochs**, indicating rapid improvement.\n  - Reached **negative values** after **10,000 epochs**, fluctuating between **-5 and 5** due to stochastic batch sampling.\n\n- **Latent Loss:**\n  - Stabilized at **30000**, much larger than the reconstruction loss but scaled down by a coefficient \"alpha,\" which approaches zero during training.\n\n---\n\n### Insights\n\n- **Reconstruction Loss Fluctuation:** Likely caused by stochastic sampling; consider tuning batch size or using gradient clipping.\n- **Latent Loss Scaling:** \"Alpha\" effectively balances reconstruction and regularization, ensuring a structured latent space.\n- **Negative Reconstruction Loss:** Indicates potential overfitting; apply dropout or increase dataset size for better generalization. --\u003e\n\u003ch3 id=\"training-testsing\"\u003eTraining testsing\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePerformance Testing (Metric: MSE)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eTest 1:\u003c/strong\u003e Predict (reconstruct) robot data from complete original data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest 2:\u003c/strong\u003e Predict (reconstruct) robot data from human data only.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest 3:\u003c/strong\u003e Predict (reconstruct) all data at ( t ) from all data at ( t - 1 ) only.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest 4:\u003c/strong\u003e Predict (reconstruct) future robot data (at ( t + 1 )) from human data only.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"additional-tests\"\u003eAdditional Tests\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTest 5:\u003c/strong\u003e Feed the human data only to predict (reconstruct) the complete data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTest 6:\u003c/strong\u003e Feed the data at ( t ) only from the previously reconstructed data, then predict the future robot data (at ( t + 1 )).\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"results-and-plots\"\u003eResults and Plots\u003c/h3\u003e\n\u003cp\u003eThe results for each test and the plots for \u003cstrong\u003eTest 4\u003c/strong\u003e (comparing the original position/velocity at ( t ) with the predicted position/velocity at ( t + 1 )) are shown below:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align:left\"\u003eTest\u003c/th\u003e\n\u003cth style=\"text-align:center\"\u003eJoint_pos_cur (at t)\u003c/th\u003e\n\u003cth style=\"text-align:center\"\u003eJoint_vel_cur (at t)\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003cstrong\u003eTest 1\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e0.0047\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e0.0091\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003cstrong\u003eTest 2\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e0.0414\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e0.0384\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003cstrong\u003eTest 3\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e0.00502\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e0.011\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align:left\"\u003e\u003cstrong\u003eTest 4\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e0.0502\u003c/td\u003e\n\u003ctd style=\"text-align:center\"\u003e0.0481\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n   \u003cimg src=\"/images/pred_act_80000p2.png\" alt=\"targets\" width=\"1000\"/\u003e\n\u003c/div\u003e\n\u003ch3 id=\"discussion\"\u003eDiscussion\u003c/h3\u003e\n\u003cp\u003eThe ultimate objective of this project is to predict future robot motion by monitoring current human motion, as depicted in \u003cstrong\u003eTest 4\u003c/strong\u003e. However, conducting the previous three tests provides valuable insights into the performance of individual components.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTest 1 and Test 2:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTest 1 demonstrates that the prediction error for robot data is minimal when using complete data as input.\u003c/li\u003e\n\u003cli\u003eTest 2 shows that while the error increases when only human data is used as input, the model still achieves excellent performance for position prediction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTest 3:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePredicting data at time ( t ) (both human and robot data) from data at time ( t - 1 ) yields performance comparable to Test 1.\u003c/li\u003e\n\u003cli\u003eThis consistency suggests that a one-time-step shift at 10 Hz does not significantly alter the state. Future work could explore incorporating larger time shifts to better evaluate prediction performance across more dynamic scenarios.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTest 4:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThis test combines the functions of Test 2 and Test 3, involving two stages of reconstruction:\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInitial Stage:\u003c/strong\u003e Human data is used to reconstruct the data at time ( t ).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSecondary Stage:\u003c/strong\u003e The reconstructed data at time ( t ) is fed as input for predicting data at time ( t - 1 ), ultimately enabling prediction of data at ( t + 1 ).\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eFrom the plots, it is evident that:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePosition Predictions:\u003c/strong\u003e Closely align with the ground truth positions.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVelocity Predictions:\u003c/strong\u003e Match the general trend of ground truth values, although some extreme points in joints 1, 2, and 4 were missed by the model.\u003c/li\u003e\n\u003cli\u003eThese missed points correspond to boundary values ((-1) or (1)) in the dataset, highlighting a potential area for improvement in the model\u0026rsquo;s handling of edge cases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOverall, these tests provide a comprehensive evaluation of the modelâ€™s prediction capabilities and outline areas for potential future enhancements.\u003c/p\u003e\n\u003ch2 id=\"related-github-repositories\"\u003eRelated GitHub Repositories\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/NuCapybara/ROS2_Myo_Franka\"\u003eROS2_Myo_Franka\u003c/a\u003e: A repository for connecting Myo armbands to the Franka Emika robot, including data collection and integrating machine learning pipelines into robot control.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/NuCapybara/mVAE_robot_human_training\"\u003emVAE_robot_human_training\u003c/a\u003e: A repository for training the processed robot and human data.\u003c/li\u003e\n\u003c/ul\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/my-fifth-post/","title":"Human-Robot Teleoperation via mVAE: Predicting Robot Motion from Human EMG and IMU Signals"},{"content":"\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n  \u003ciframe src=\"https://www.youtube.com/embed/M1ve4sudqlI\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen title=\"YouTube Video\"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"heading\"\u003e\u003c/h2\u003e\n\u003ch2 id=\"project-overview\"\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eThis 10-week project aims to develop a small, affordable robot that can recognize lip movements and accurately apply a cotton swab to the lips. It\u0026rsquo;s designed to help patients who are unconscious and suffering from dry lips and mouth due to their inability to drink water on their own. Typically, caregivers moisten the patient\u0026rsquo;s lips with a moist cotton swab to hydrate and clean the area, which is a repetitive task that requires constant attention. By automating this process, the robot can alleviate the burden on healthcare workers and caregivers, freeing them up to focus on other important duties.\u003c/p\u003e\n\u003ch2 id=\"overall-system\"\u003eOverall System\u003c/h2\u003e\n\u003cp\u003eThe image below shows the main nodes in the system and how they communicate to achieve a task flow.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/winter_flow_chart.png\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003ch2 id=\"hardware\"\u003eHardware\u003c/h2\u003e\n\u003cp\u003eThe system is composed of an interbotix WX-200 robot and an Intel RealSense D435i Depth camera. The connected black 3D parts are designed for precise calibration between camera and robot.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/robot_rs4.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003ch2 id=\"computer-vision\"\u003eComputer Vision\u003c/h2\u003e\n\u003cp\u003eThe application\u0026rsquo;s computer vision utilizes the dlib library for facial landmark detection with deep learning, using a shape predictor model trained on a large dataset to accurately identify facial features.\u003c/p\u003e\n\u003cp\u003eFacial landmarks are converted from 2D to 3D using the Intel RealSense camera\u0026rsquo;s depth sensing, aligning color and depth frames for accurate depth information. The set of landmarks are then published as coordinates in a PoseArray for the Robot control Node.\u003c/p\u003e\n\u003c!-- ![target](/images/oldman_rviz_image2.png) --\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cimg src=\"/images/oldman_rviz_image2.png\" alt=\"Computer Vision\"\u003e\n\u003c/div\u003e\n\u003cp\u003eThe circular image above displays the camera\u0026rsquo;s detection of 2D lip landmarks. In contrast, the simulation incorporating depth clouds illustrates the lip points as arrays in 3D coordinates, marked by light blue spheres. During execution, the designated target points are indicated by red sphere markers. A static transformation between the robot and camera, based on the head-link frame representing the leftmost lip point on a male mannequin, allows the robot to accurately locate lip points.\u003c/p\u003e\n\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cimg src=\"/images/compare_wig_3.gif\" alt=\"Mouth Care Process Accuracy\"\u003e\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    The mouth care process maintains high accuracy across various facial structures.\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cimg src=\"/images/compare_wig_2.gif\" alt=\"Adapting to Face Position Changes\"\u003e\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    The mouth care process adapts to changes in the face's position and angle.\n\u003c/div\u003e\n\u003ch2 id=\"future-work\"\u003eFuture Work\u003c/h2\u003e\n\u003cp\u003eThe face landmarks detection goes beyond identifying lip points, also capturing features like eyes and nose. This opens avenues for extending the application to tasks like facial cleaning, feeding, or rehabilitation processes. Please feel free to use my code in GitHub for future extended works :)\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/my-fourth-post/","title":"Robotic Assistant for Lip Moisturizing: Enhancing Care for Unconscious Patients"},{"content":"\u003cdiv style=\"text-align:center;\"\u003eAuthors: Ethan Parham, Jialu Yu, David Morris, Henry Meiselman\u003c/div\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/Intro_whale_1.webp\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eTagging whales enables us to characterize their health and lifestyle, allowing us as a species to adjust our actions and minimize our impact on these animals. Drone tagging proves superior to traditional boat tagging methods, as it reduces the time and costs associated with data collection and enables the tagging of smaller, more agile animals.\u003c/p\u003e\n\u003cp\u003eOne of the main challenges is the tags missing the whales; currently, the drone needs to be around 16 to 20 feet in the air to ensure the tag has the correct orientation to stick, negatively impacting drop accuracy. Another concern is whether prop wash affects the release of the tags.\u003c/p\u003e\n\u003cp\u003eTo address these issues, we will develop a \u003cstrong\u003ephysics-based model for dropping tags from drones\u003c/strong\u003e. This model will guide the design of the optimal release mechanism, ensuring the necessary impulse, orientation, and accuracy for successful attachment and target hits.\u003c/p\u003e\n\u003ch2 id=\"user-requirements-and-engineering-specifications\"\u003eUser Requirements and Engineering Specifications\u003c/h2\u003e\n\u003cp\u003eThe project requirements and specifications were formulated through discussions with the ocean research team and extensive research into considerations essential for a payload delivery system. These requirements are categorized into three aspects: physical requirements, performance requirements, and usage requirements. The user requirements and engineering specifications serve as the foundation for our design, guiding the development of an effective payload delivery system.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/user_spec.png\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003ch2 id=\"build-description\"\u003eBuild Description\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eDesign of Release Mechanism\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/release_design.png\" alt=\"targets\"\u003e\n\u003cimg src=\"/images/DTAG.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Figure above shows the CAD assembly of the release mechanism design and how it is installed onto the fin, clamp and the tag.\u003c/p\u003e\n\u003cp\u003eThe release mechanism is made up of four 3D printed components. The servo mount and spring mount connect to the drone, while the fin topper and fin topper disk connect to the top of the fins. The servo motor, mounted in the servo mount, drives a linkage which is used to pull the servo pin. When the pin is pulled, a spring mounted into the spring mount is uncompressed, which forces the tag assembly downwards. The tag assembly consists of the fin topper, fin topper disk, fin, clamp, and tag. The clamp and tag used in the testing are unmodified parts which were provided to us by the research team.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/release_design2.png\" alt=\"targets\"\u003e\n\u003cimg src=\"/images/release_design3.png\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eThe two figures above illustrates the release mechanism before and after the pin is pulled. The compressed spring on the left image is not depicted for clarity. The spring is initially compressed, until the servo pin can be pushed into the catch on the fin topper. Then, when the pin is pulled the spring is uncompressed and the tag is launched downwards. The fin will be mounted to the bottom of the topper, in an assembly similar to the one shown below, and the additional initial velocity from releasing the energy of the spring allows the tag to travel faster than the drone downwash. As a result, we believe the tag will be more stable in flight since higher velocity air moving over the fins will create a larger corrective moment.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4 New Fin Designs\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/new_fin.png\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eWe additionally created four new fin designs to test in addition to the original design used. The new designs, shown above, are primarily variations on the original design which use the same head for mounting the DTAG clamp and hole for inserting the fin topper.\u003c/p\u003e\n\u003cp\u003eThe intent of the rotated fin was to move the fins into the cross-shaped region of less turbulent lower velocity air between the drone rotors. The trapezoidal fin shape mimics the shape of a dart fin, and the angled back may make the fin more stable at lower tag velocities when the downwash flows past the tag. The larger and smaller fin designs were created to test the effect changing the surface area of the fin had on the tagâ€™s stability.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAll Components in a glance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/everything2.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eAll components engineered are visually represented in the graph below.\u003c/p\u003e\n\u003ch2 id=\"verification-plan\"\u003eVerification Plan\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ePart I: Orientation Angle\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/rig1.png\" alt=\"targets\"\u003e\n\u003cimg src=\"/images/rig2.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eThe orientation of the tag while falling through the air must remain less than 25Â° to meet our specification. We will verify this by using the mechanism to drop the tag from a test rig which is modeled in the figure below.\u003c/p\u003e\n\u003cp\u003eWe will record these drop tests and use the software Tracker to measure the maximum orientation angle of each drop. A screenshot of the Tracker software can be seen in Figure 32 below. The orientation angle over time is plotted in the top right corner of Figure 32. We can verify whether the orientation angle is less than 25Â° for each drop by looking at this plot.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePart II: Impact Force\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe impulsive impact force must be between 5 and 26.6 Ns in order to meet our specifications. We will verify this in a similar way as we verified the orientation angle, by using the Tracker software on a video recording of a physical test drop. The Tracker software in this case will measure the velocity of the tag during the descent. A screenshot of the Tracker software measuring velocity can be seen in figure below. We can plug the final velocity at impact into equation to determine the impulsive impact force based on this final velocity. From here we can verify whether the impulsive impact force of each drop test meets our specification of being in between 5 and 26.6 Ns.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/test_1.png\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eFin Chosen\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/test3.gif\" alt=\"targets\"\u003e\n\u003cimg src=\"/images/test4.gif\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eThe experiment results reveal that Fin 2, featuring the small fin, achieves the necessary impact forces at 12.4 N while keeping the tag orientation deviation within a favorable range of 3.2-10 degrees. This outcome underscores the effectiveness of our design in ensuring both adequate impact forces and minimal deviation in tag orientation. To visually showcase the performance of Fin 2, we have provided a GIF illustrating the test results for this specific fin design.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRelease Mechanism with Fin 2\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe accompanying image displays the final product, showcasing the release mechanism installed with the servo motor and Fin 2, reflecting the successful culmination of our efforts in creating a reliable and impactful drone tagging system.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/tag_whole.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/my-third-post/","title":"Drone Tagging"},{"content":"\u003cdiv style=\"text-align:center;\"\u003eAuthors: Joel Goh, Maximiliano Palay, Rahul Roy, Sophia Schiffer, Jialu Yu\u003c/div\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n  \u003ciframe src=\"https://www.youtube.com/embed/HQIWntieInI\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen title=\"YouTube Video\"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003cp\u003eIn this project, the Franka arm searches for known randomly placed pins and shoots them down. The Franka arm is fitted with an onboard camera to search its environment and two Nerf blasters to be able to shoot the targets. There are two rounds of shooting, while during each rounds, the user can verbally specify colors of the pins that the arm is going to shoot and the Franka arm will pick up one gun to shoot all the designate colored pins at the environment.\u003c/p\u003e\n\u003ch2 id=\"overall-system\"\u003eOverall System\u003c/h2\u003e\n\u003cp\u003eThe image below shows the different nodes in the system and how they communicate with each other. Each node\u0026rsquo;s funtionalities will be explained below.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/bowling_nodes.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003ch2 id=\"nodes\"\u003eNodes\u003c/h2\u003e\n\u003cp\u003eCrafted by our team within the ROS(Robot Operating System), this project features several key components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eControl Node\n\u003cul\u003e\n\u003cli\u003eActs as the main hub, coordinating tasks by calling services from other nodes to complete the demo.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMoveGun Node\n\u003cul\u003e\n\u003cli\u003eManages path planning and robot movement, handling everything from body to gripper. It uses MoveIt-interface services like Cartesian planning, IK planning, and gripper requests.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eYolo Node\n\u003cul\u003e\n\u003cli\u003eRuns YOLO (Real-Time Object Detection System) to find colored bowling pins in relation to the base of the Franka arm. Displays their positions as markers in Rviz2, a user-friendly graphical interface.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUser_Input Node\n\u003cul\u003e\n\u003cli\u003eListens to the user\u0026rsquo;s audio input and commands the robot about the color of the targeted pins, making the system responsive to user instructions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTrigger\n\u003cul\u003e\n\u003cli\u003eControls the Arduino to automate the gun\u0026rsquo;s trigger, adding automation and precision to the project.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eApriltag_node\n\u003cul\u003e\n\u003cli\u003eCreated by the University of Michigan Team.\u003c/li\u003e\n\u003cli\u003eThis node scans and provides coordinates for April tags. We use it to help the robot grip the gun by accurately detecting its position. The AprilTag technology ensures precise and reliable positioning in our project.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"launch-files\"\u003eLaunch Files\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eShoot_pins.launch.xml in control package\n\u003cul\u003e\n\u003cli\u003eThis launch file starts up the six nodes listed above and RViz node to help visualize the target pins and robot movement simulation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"quickstart-to-reproduce-the-project\"\u003eQuickstart to reproduce the project\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eConnect the Arduino and RealSense camera to the computer\n\u003cul\u003e\n\u003cli\u003eOptional to connect an external microphone to your computer but the built-in computer microphone works as well\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEnsure that the two nerf guns are in front of the Franka arm roughly symmetrical around the y axis\n\u003cul\u003e\n\u003cli\u003eThe two guns need the apriltag 42 and 95 from the 36h11 family\u003c/li\u003e\n\u003cli\u003eEach gun is also loaded with 6 bullets\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStart the moveit group on the Franka station using \u003ccode\u003eros2 launch franka_moveit_config moveit.launch.py use_rviz:=false robot_ip:=panda0.robot\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eConnect to the realsense by running on your computer \u003ccode\u003eros2 launch realsense2_camera rs_launch.py depth_module.profile:=1280x720x30 pointcloud.enable:=true align_depth.enable:=true\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eOn your computer run the launch file, \u003ccode\u003eros2 launch control shoot_pins.launch.xml\u003c/code\u003e, which starts the rest of the necessary nodes\n\u003cul\u003e\n\u003cli\u003eAn error message will show with the msg, \u0026ldquo;Waiting for input\u0026rdquo;, where the audio input can be given starting the demo\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/my-first-post/","title":"Franka Robot: Precision Pin-hunting Meets Playfulness"},{"content":"\u003c!-- \u003cdiv style=\"text-align:center;\"\u003eAuthors: Joel Goh, Maximiliano Palay, Rahul Roy, Sophia Schiffer, Jialu Yu\u003c/div\u003e --\u003e\n\u003c!-- ## Publications: --\u003e\n\u003cbr\u003e\n\u003c!-- \n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n  \u003ciframe src=\"https://www.youtube.com/embed/HQIWntieInI\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen title=\"YouTube Video\"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n   --\u003e\n\u003ch3 id=\"1-conflict-free-node-to-robot-scheduling-for-lifelong-operation-in-a-warehouse-with-narrow-corridor-environment\"\u003e1. Conflict-Free Node-to-Robot Scheduling for Lifelong Operation in a Warehouse with Narrow-Corridor Environment.\u003c/h3\u003e\n\u003cdiv style=\"text-align: center; margin-bottom: 20px;\"\u003e\n    \u003cimg src=\"/images/publication/Overview_pic.png\" alt=\"Robot Fleet task scheduling algorithm\" width=\"400\" height=\"auto\"\u003e\n\u003c/div\u003e\n\u003ch5 id=\"--published-in-ieee-conference-on-decision-and-control-cdc-2023httpsieeexploreieeeorgdocument10384309\"\u003e- \u003cstrong\u003e\u003ca href=\"https://ieeexplore.ieee.org/document/10384309\"\u003ePublished in IEEE Conference on Decision and Control (CDC) 2023\u003c/a\u003e\u003c/strong\u003e\u003c/h5\u003e\n\u003ch5 id=\"--affiliation-addverbhttpsimgshieldsiobadgeaddverb-redstyleflatscale5httpsaddverbcom\"\u003e- Affiliation: \u003ca href=\"https://addverb.com/\"\u003e\u003cimg src=\"https://img.shields.io/badge/Addverb-red?style=flat\u0026amp;scale=5\" alt=\"Addverb\"\u003e\u003c/a\u003e\u003c/h5\u003e\n\u003cbr\u003e\n\u003c!-- Some format Options --\u003e\n\u003c!-- - Link: https://ieeexplore.ieee.org/document/10384309 --\u003e\n\u003c!-- For colouring parts --\u003e\n\u003c!-- - \u003cspan style=\"color:red;\"\u003e[Addverb]\u003c/span\u003e(https://addverb.com/) --\u003e\n\u003c!-- for markdown linking --\u003e\n\u003c!-- - Affiliation: [Addverb][Addverb] --\u003e\n\u003c!-- for html linking --\u003e\n\u003c!-- - Affiliation: \u003ca href=\"https://addverb.com/\" style=\"color:red;\"\u003eAddverb\u003c/a\u003e --\u003e\n\u003c!-- for iconised link --\u003e\n\u003c!-- - Affiliation: [![Addverb](https://img.shields.io/badge/Addverb-red)](https://addverb.com/) --\u003e\n\u003c!-- for bold and hover link --\u003e\n\u003c!-- - Affiliation: **[Addverb](https://addverb.com/)** --\u003e\n\u003c!-- custom icon, slightly scaled --\u003e\n\u003ch3 id=\"2-multiple-damage-detection-in-pzt-sensor-using-dual-point-contact-method\"\u003e2. Multiple Damage Detection in PZT Sensor Using Dual Point Contact Method.\u003c/h3\u003e\n\u003cdiv style=\"text-align: center; margin-bottom: 20px;\"\u003e\n    \u003cimg src=\"/images/publication/Setup_fig1.png\" alt=\"Multiple Damage Detection \" width=\"400\" height=\"auto\"\u003e\n\u003c/div\u003e\n\u003ch5 id=\"--published-in-mdpi-sensors-2022httpswwwmdpicom1424-822022239161\"\u003e- \u003cstrong\u003e\u003ca href=\"https://www.mdpi.com/1424-8220/22/23/9161\"\u003ePublished in MDPI sensors 2022\u003c/a\u003e\u003c/strong\u003e\u003c/h5\u003e\n\u003ch5 id=\"--affiliation-the-arctic-university-of-norwayhttpsimgshieldsiobadgearctic_univeristy-norway-bluestyleflatscale5httpsenuitnostartsida\"\u003e- Affiliation: \u003ca href=\"https://en.uit.no/startsida\"\u003e\u003cimg src=\"https://img.shields.io/badge/Arctic_Univeristy-Norway-blue?style=flat\u0026amp;scale=5\" alt=\"The Arctic University of Norway\"\u003e\u003c/a\u003e\u003c/h5\u003e\n\u003cbr\u003e\n\u003ch3 id=\"3-multiple-damage-detection-in-piezoelectric-ceramic-sensor-using-scanning-point-contact-excitation-and-detection-method\"\u003e3. Multiple damage detection in piezoelectric ceramic sensor using scanning point contact excitation and detection method.\u003c/h3\u003e\n\u003cdiv style=\"text-align: center; margin-bottom: 20px;\"\u003e\n    \u003cimg src=\"/images/publication/Setup_fig3.png\" alt=\"Multiple Damage Detection \" width=\"400\" height=\"auto\"\u003e\n\u003c/div\u003e\n\u003ch5 id=\"--presented-in-symposium-of-ultrasonic-electric-japan-2021httpswwwuse-dlorg2021proceedingspdf2pb2-2pdf\"\u003e- \u003cstrong\u003e\u003ca href=\"https://www.use-dl.org/2021/proceedings/pdf/2Pb2-2.pdf\"\u003ePresented in Symposium of Ultrasonic Electric Japan 2021\u003c/a\u003e\u003c/strong\u003e\u003c/h5\u003e\n\u003ch5 id=\"--affiliation-the-arctic-university-of-norwayhttpsimgshieldsiobadgearctic_univeristy-norway-bluestyleflatscale5httpsenuitnostartsida-1\"\u003e- Affiliation: \u003ca href=\"https://en.uit.no/startsida\"\u003e\u003cimg src=\"https://img.shields.io/badge/Arctic_Univeristy-Norway-blue?style=flat\u0026amp;scale=5\" alt=\"The Arctic University of Norway\"\u003e\u003c/a\u003e\u003c/h5\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/publications/","title":"Publications:"}]